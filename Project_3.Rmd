---
title: "Project_3"
author: "Aaron Zelmanov, Muhammad Hafizudeen Mohamad Saman, Nakul Chadha, Kendall Cohen, Michael Geraci"
date: "3/29/2021"
output:
  html_document:
    code_folding: hide
---
## {.tabset}

### Introduction 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(class)
library(caret)
library(gmodels)
library(InformationValue)
library(C50)
library(neuralnet)
library(InformationValue)

airline_train <- read.csv("train.csv")
airline_test <- read.csv("test.csv")

airline_train$X <- NULL
airline_train$id <- NULL
airline_train$Gender <- as.factor(airline_train$Gender)
airline_train$Customer.Type <- as.factor(airline_train$Customer.Type)
airline_train$Type.of.Travel <- as.factor(airline_train$Type.of.Travel)
airline_train$Class <- as.factor(airline_train$Class)
airline_train$satisfaction <- as.numeric(as.factor(airline_train$satisfaction))-1

airline_test$X <- NULL
airline_test$id <- NULL
airline_test$Gender <- as.factor(airline_test$Gender)
airline_test$Customer.Type <- as.factor(airline_test$Customer.Type)
airline_test$Type.of.Travel <- as.factor(airline_test$Type.of.Travel)
airline_test$Class <- as.factor(airline_test$Class)
airline_test$satisfaction <- as.numeric(as.factor(airline_test$satisfaction))-1
```
**Context:** We are working with the airline dataset which contains various demographics and flight information along with the final satisfaction level.  

**Audience:** The results of our models are useful for airlines in order to optimize their flight experience, lead to maximum satisfaction, and determine which passengers will be satisfied. 

**Key Business Objectives:** What factors are predictive of a satisfied or dissatisfied passenger? How can airlines use this information to improve certain aspects of service? How can airlines predict who to invest more in (dissatisfied passengers)? 

```{r}
#Train and Test sets to use 

#airline_train FOR decision tree, regression
#airline_test FOR decision tree, regression
#airline_train_factorized_normalized FOR KNN, ANN
#airline_test_factorized_normalized FOR KNN, ANN

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x))) #everything 0 to 1
}

airline_train_factorized<- as.data.frame(model.matrix(~.-1, data = airline_train))
airline_train_factorized_normalized <- as.data.frame(lapply(airline_train_factorized[1:ncol(airline_train_factorized)], normalize))
airline_factorizednormalized_trainlabels <- airline_train_factorized_normalized["satisfaction"]

airline_test_factorized<- as.data.frame(model.matrix(~.-1, data = airline_test))
airline_test_factorized_normalized <- as.data.frame(lapply(airline_test_factorized[1:ncol(airline_test_factorized)], normalize))
airline_factorizednormalized_testlabels <- airline_test_factorized_normalized["satisfaction"]
```

### Data Exploration 

The goal of this analysis is to identify opportunities for which metrics this airline can improve and how that would affect the overall satisfaction of their customers. We want to look at the elements that are the most and least likely to induce a positive or neutral/negative reaction in a customer. This way, we can identify the elements that the airline needs to improve on and where they need to spend the most amount of their resources (time/money).

```{r}
#Correlations with satisfaction 
#correlations <- as.data.frame(cor(airline_train[sapply(airline_train, function(x) is.numeric(x))])[19,])
#correlations %>% ggplot(aes(x=reorder(rownames(correlations), correlations[,1]), y=correlations[,1])) + 
#geom_bar(stat = 'identity') + ggtitle("Correlations with Satisfaction") + labs(x="Variables", y="Correlation") + theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_text(angle = 90))

```

Now, let's take a look at the variables that could have a significant effect on our response.

```{r}
ggplot(data = airline_train_factorized, aes(x = Inflight.wifi.service, y = satisfaction, colour = satisfaction)) + geom_jitter() + xlab("Inflight Wifi Service") + ylab("Satisfaction") + ggtitle("Inflight Wifi Service vs Satisfaction") + theme(plot.title = element_text(hjust = 0.5))
```

### Regression 

```{r}
logit_model1 <- glm(satisfaction ~., data = airline_train, family = "binomial")
summary(logit_model1)

prediction <- predict(logit_model1, newdata = airline_test, type = "response")

#optimal <- optimalCutoff(airline_test$satisfaction, prediction)[1]

#binary_predictions <- ifelse(prediction > optimal, 1, 0)

#airline_test_labels <- airline_test[airline_test, "satisfaction"]

#CrossTable(x = airline_test$satisfaction, y = binary_predictions, prop.chisq = FALSE)
```

### KNN

```{r}

```


### ANN

```{r cache = TRUE}
airlineneuralnet1 <- neuralnet(formula = satisfaction ~ ., data = airline_train_factorized_normalized, hidden = 2)

model_results <- compute(airlineneuralnet1, airline_test_factorized_normalized)
predicted_y <- model_results$net.result
summary(predicted_y)

model_pred <- predict(airlineneuralnet1, newdata = airline_test_factorized_normalized, type = "response")
cutoff <- optimalCutoff(airline_test_factorized_normalized$satisfaction, model_pred)[1]
predicted_ANN_results <- ifelse(model_pred > cutoff, 1, 0) 

caret::confusionMatrix(as.factor(predicted_ANN_results), as.factor(airline_factorizednormalized_testlabels$satisfaction)) #WHY $ HERE??
```

The neural net algorithm provides a prediction of satisfaction by using nodes that model the neurons in our brain. To optimize this model, I adjusted the number of hidden nodes and the probability cutoff for a yes (1) prediction. I found that adding hidden nodes helped with accuracy. Going to 2 hidden nodes increased the accuracy to 91%. However, adding any more hidden nodes than 2 proved to be too computationally demanding for the computer to execute. Thereafter, I employed the optimalCutoff function to determine that a cutoff of 41.9% was the optimal value for maximizing accuracy. At this cutoff, sensitivity remains high too. 

### Decision Tree 

```{r}
airline_test$satisfaction <- as.factor(airline_test$satisfaction)
airline_train$satisfaction <- as.factor(airline_train$satisfaction)

airline_decision <- C5.0(satisfaction ~ ., data = airline_train)
airline_decision_pred <- predict(airline_decision, newdata = airline_test)

CrossTable(airline_test$satisfaction, airline_decision_pred)
airline_decision_pred <- as.factor(airline_decision_pred)
caret::confusionMatrix(airline_decision_pred, airline_test$satisfaction)
```

### SVM 

```{r}

svm_model1 <- ksvm(satisfaction ~., data = airline_train, kernel = "vanilladot")
```

### Combined Model 

```{r}

```


### Conclusion 
