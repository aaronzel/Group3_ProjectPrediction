---
title: "Project_3"
author: "Aaron Zelmanov, Muhammad Hafizudeen Mohamad Saman, Nakul Chadha, Kendall Cohen, Michael Geraci"
date: "3/29/2021"
output:
  html_document:
    code_folding: hide
---
## {.tabset}

### Introduction 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(class)
library(caret)
library(gmodels)
library(InformationValue)
library(C50)
library(neuralnet)
library(InformationValue)

airline_train <- read.csv("train.csv")
airline_test <- read.csv("test.csv")

airline_train$X <- NULL
airline_train$id <- NULL
airline_train$Gender <- as.factor(airline_train$Gender)
airline_train$Customer.Type <- as.factor(airline_train$Customer.Type)
airline_train$Type.of.Travel <- as.factor(airline_train$Type.of.Travel)
airline_train$Class <- as.factor(airline_train$Class)
airline_train$satisfaction <- as.numeric(as.factor(airline_train$satisfaction))-1

airline_test$X <- NULL
airline_test$id <- NULL
airline_test$Gender <- as.factor(airline_test$Gender)
airline_test$Customer.Type <- as.factor(airline_test$Customer.Type)
airline_test$Type.of.Travel <- as.factor(airline_test$Type.of.Travel)
airline_test$Class <- as.factor(airline_test$Class)
airline_test$satisfaction <- as.numeric(as.factor(airline_test$satisfaction))-1

```
**Context:** We are working with the airline dataset which contains various demographics and flight information along with the final satisfaction level.  

**Audience:** The results of our models are useful for airlines in order to optimize their flight experience, lead to maximum satisfaction, and determine which passengers will be satisfied. 

**Key Business Objectives:** What factors are predictive of a satisfied or dissatisfied passenger? How can airlines use this information to improve certain aspects of service? How can airlines predict who to invest more in (dissatisfied passengers)? 

```{r}
#Train and Test sets to use 

#airline_train FOR decision tree, regression
#airline_test FOR decision tree, regression
#airline_train_factorized_normalized FOR KNN, ANN
#airline_test_factorized_normalized FOR KNN, ANN

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x))) #everything 0 to 1
}

airline_train_factorized<- as.data.frame(model.matrix(~.-1, data = airline_train))

airline_train_factorized_normalized <- as.data.frame(lapply(airline_train_factorized[1:ncol(airline_train_factorized)], normalize))

airline_factorizednormalized_trainlabels <- airline_train_factorized_normalized["satisfaction"]

airline_test_factorized<- as.data.frame(model.matrix(~.-1, data = airline_test))
airline_test_factorized_normalized <- as.data.frame(lapply(airline_test_factorized[1:ncol(airline_test_factorized)], normalize))
airline_factorizednormalized_testlabels <- airline_test_factorized_normalized["satisfaction"]

# DO WE NEED TO ADD THIS 
# airline_test_factorized_normalized$satisfaction <- NULL 
# airline_train_factorized_normalized$satisfaction <- NULL 


```

### Data Exploration 

The goal of this analysis is to identify opportunities for which metrics this airline can improve and how that would affect the overall satisfaction of their customers. We want to look at the elements that are the most and least likely to induce a positive or neutral/negative reaction in a customer. This way, we can identify the elements that the airline needs to improve on and where they need to spend the most amount of their resources (time/money).

```{r}
#Correlations with satisfaction 
#correlations <- as.data.frame(cor(airline_train[sapply(airline_train, function(x) is.numeric(x))])[19,])
#correlations %>% ggplot(aes(x=reorder(rownames(correlations), correlations[,1]), y=correlations[,1])) + 
#geom_bar(stat = 'identity') + ggtitle("Correlations with Satisfaction") + labs(x="Variables", y="Correlation") + theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_text(angle = 90))

```

Now, let's take a look at the variables that could have a significant effect on our response.

```{r}
ggplot(data = airline_train_factorized, aes(x = Inflight.wifi.service, y = satisfaction, colour = satisfaction)) + geom_jitter() + xlab("Inflight Wifi Service") + ylab("Satisfaction") + ggtitle("Inflight Wifi Service vs Satisfaction") + theme(plot.title = element_text(hjust = 0.5))
```

### Regression 

```{r}
logit_model1 <- glm(satisfaction ~., data = airline_train, family = "binomial")
summary(logit_model1)

prediction <- predict(logit_model1, newdata = airline_test, type = "response")

#optimal <- optimalCutoff(airline_test$satisfaction, prediction)[1]

#binary_predictions <- ifelse(prediction > optimal, 1, 0)

#airline_test_labels <- airline_test[airline_test, "satisfaction"]

#CrossTable(x = airline_test$satisfaction, y = binary_predictions, prop.chisq = FALSE)
```

### KNN

```{r}
set.seed(12345)

# Run KNN command 
library(class)
library(caret)

# Determine k 
sqrt(nrow(airline_train_factorized_normalized))

str(airline_train_factorized_normalized)
str(airline_factorizednormalized_trainlabels)

satifaction_test_pred_knn <- knn(train = airline_train_factorized_normalized, test = airline_test_factorized_normalized, cl = airline_factorizednormalized_trainlabels, k = 321)

# Cross table 
CrossTable(x = airline_factorizednormalized_testlabels, y = satifaction_test_pred_knn, 
           prop.chisq=FALSE)

# Confusion Matrix
confusionMatrix(as.factor(satifaction_test_pred_knn), as.factor(airline_factorizednormalized_testlabels))
```


### ANN

```{r cache = TRUE}
airlineneuralnet1 <- neuralnet(formula = satisfaction ~ ., data = airline_train_factorized_normalized, hidden = 2)

model_results <- compute(airlineneuralnet1, airline_test_factorized_normalized)
predicted_y <- model_results$net.result
summary(predicted_y)

model_pred <- predict(airlineneuralnet1, newdata = airline_test_factorized_normalized, type = "response")
cutoff <- optimalCutoff(airline_test_factorized_normalized$satisfaction, model_pred)[1]
predicted_ANN_results <- ifelse(model_pred > cutoff, 1, 0) 

caret::confusionMatrix(as.factor(predicted_ANN_results), as.factor(airline_factorizednormalized_testlabels$satisfaction)) #WHY $ HERE??
```

The neural net algorithm provides a prediction of satisfaction by using nodes that model the neurons in our brain. To optimize this model, I adjusted the number of hidden nodes and the probability cutoff for a yes (1) prediction. I found that adding hidden nodes helped with accuracy. Going to 2 hidden nodes increased the accuracy to 91%. However, adding any more hidden nodes than 2 proved to be too computationally demanding for the computer to execute. Thereafter, I employed the optimalCutoff function to determine that a cutoff of 41.9% was the optimal value for maximizing accuracy. At this cutoff, sensitivity remains high too. 

### Decision Tree 

```{r}
airline_test$satisfaction <- as.factor(airline_test$satisfaction)
airline_train$satisfaction <- as.factor(airline_train$satisfaction)

airline_decision <- C5.0(satisfaction ~ ., data = airline_train)
airline_decision_pred <- predict(airline_decision, newdata = airline_test)

CrossTable(airline_test$satisfaction, airline_decision_pred)
airline_decision_pred <- as.factor(airline_decision_pred)
caret::confusionMatrix(airline_decision_pred, airline_test$satisfaction)
```

### SVM 

Now, let's predict the data using Support Vectors Machine (SVM). SVM is a supervised learning method that tries to draw a boundary between two levels of response while maximizing margin and minimizing the distance of the misclassified errors from the best hyperplane (or the separating boundary). There are many different types of boundaries, each type corresponds to a different kernel, which is a function that computes the inner product between our predictors. The most popular kernals are the Radial Basis kernel, Polynomial kernel, and Linear kernel. However, non-linear kernals are computationally expensive to run, so we are going to run only the linear kernal. Note that the linear kernal corresponds to the parameter option "vanilladot" in kernlab library.

```{r}
library(kernlab)
#building SVM model
svm_model1 <- ksvm(as.factor(satisfaction) ~., data = airline_train, kernel = "vanilladot", scaled = TRUE)
svm_model1

#making predictions
predictions_svm <- predict(svm_model1, airline_test)

#evaluating model performance
head(predictions_svm)
summary(predictions_svm)

#table(predictions_svm, airline_test$satisfaction)
#confusionMatrix(as.factor(predictions_svm), as.factor(airline_test$satisfaction))
#fix NA values to get this
```

### Combined Model 

```{r}

```


### Conclusion 
